{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3799c295",
   "metadata": {},
   "source": [
    "## Data wrangle report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f1644",
   "metadata": {},
   "source": [
    "#### Performing Data wrangling bascially involve three stage as describe; \n",
    "\n",
    "\n",
    "Firstly,i had to source and acquire the needed data set that would be of need in carrying out my analysis. So, i had to import the libraries needed in carrying out this operation such as pandas, numpy, request, os, matplotlib. Thereafter, i downloaded the first flat file (twitter_archive_csv) directly from the udacity project page, and also the two other file (image_prediction.tsv, & tweet_json). I read these files into a pandas DataFrame and assigned them a unique variable name of my choice, but i had to specifically obtain particu some specific columns for the \"tweet_json\" dataset according to the instructions given using pandas function. \n",
    "\n",
    "Secondly, i began to access my data sets for \"Quality and Tidness\". i performed this act using the visual assessment and programmatic assessment. in the visual assessment i was able to spot some quality issues such as 'none' in the \"name\" colum  of the twiiter_archive dataset, in the imcomplete spelling of the column name \"p1_conf\" in the image_prediction dataset. Also, i performed some programmatic assessment where i used pandas data functions and methods (.info(), .head(), e.t.c.) to check out for more quality and tideness issues. I had to check and iterate these particular steps because i had to look out for each column in the various dataset. \n",
    "\n",
    "Thirdly, after much assessment, i started cleaning each quality and tidness issues that i have identifed one at a time by first; (i)stating the issue(\"this made my aware of the problem i am to worked on\"), (ii) defining on how to carry out the cleaning operation(it actually gave me sense of actions and the things to do to make the cleaning process), (iii) writing the code( using some. At this stage, i had to put in much effort and thinking on how to specifically clean these errors neatly to produce a good desirable error free data. \n",
    ".\n",
    "Lastly, passing through these wrangling processes i was able to merge all three dataset together to have a clean data which can be used for futhert analysis and visualization, But before this, i stored this clean single data set into a pandas Dataframe called \"twitter_archive_master_csv\". Documenting these steps, actually made my wrangling process pretty easy, smooth and enjoyable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
